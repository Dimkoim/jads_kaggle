{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocessing import ohe_explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'},\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    # Normalize JSON columns\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = pd.io.json.json_normalize(df[column])\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    \n",
    "    # Parse date\n",
    "    df['date'] = df['date'].apply(lambda x: pd.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    print(\"Loaded file {}\\nShape is: {}\".format(path, df.shape))\n",
    "    return df\n",
    "\n",
    "def process(train, test):\n",
    "    print(\"Dropping constant columns...\")\n",
    "    \n",
    "    # Remove columns with constant values.\n",
    "    const_cols = [c for c in train.columns if train[c].nunique(dropna=False) == 1]\n",
    "    train = train.drop(const_cols, axis=1)\n",
    "    test = test.drop(const_cols, axis=1)\n",
    "    \n",
    "    # Cast target\n",
    "    train[\"transactionRevenue\"] = train[\"transactionRevenue\"].fillna(0).astype(float)\n",
    "    train[\"target\"] = np.log(train[\"transactionRevenue\"] + 1)\n",
    "    del train[\"transactionRevenue\"]\n",
    "    \n",
    "    train_len = train.shape[0]\n",
    "    merged = pd.concat([train, test], sort=False)\n",
    "\n",
    "    # Change values as “not available in demo dataset”, “(not set)”, “unknown.unknown”, “(not provided)” to nan\\n\",\n",
    "    list_missing=[\"not available in demo dataset\", \"(not provided)\", \"(not set)\", \"<NA>\", \"unknown.unknown\",  \"(none)\"]\n",
    "    merged=merged.replace(list_missing, np.nan)\n",
    "\n",
    "    # Create some features.\n",
    "    merged['diff_visitId_time'] = merged['visitId'] - merged['visitStartTime']\n",
    "    merged['diff_visitId_time'] = (merged['diff_visitId_time'] != 0).astype(int)\n",
    "    del merged['visitId']\n",
    "    del merged['sessionId']\n",
    "\n",
    "    print(\"Generating date columns...\")\n",
    "    merged['WoY'] = merged['date'].apply(lambda x: x.isocalendar()[1])\n",
    "    merged['month'] = merged['date'].apply(lambda x: x.month)\n",
    "    merged['quarterMonth'] = merged['date'].apply(lambda x: x.day // 8)\n",
    "    merged['weekday'] = merged['date'].apply(lambda x: x.weekday())\n",
    "    del merged['date']\n",
    "\n",
    "    format_time = lambda t: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n",
    "    merged['visitHour'] = pd.to_datetime(merged['visitStartTime'].apply(format_time)).apply(lambda t: t.hour)\n",
    "    del merged['visitStartTime']\n",
    "    \n",
    "    \n",
    "    print(\"Finding total visits...\")\n",
    "    # This could be considered an information leak as I am including information about the future when predicting\n",
    "    # the revenue of a transaction. In reality, when looking at the 3rd visit we would have no way of knowning\n",
    "    # that the user will actually shop X more times (or if he will visit again at all). However since this\n",
    "    # info also exists in the test set we might use it.\n",
    "    total_visits = merged[[\"fullVisitorId\", \"visitNumber\"]].groupby(\"fullVisitorId\", as_index=False).max()\n",
    "    total_visits.rename(columns={\"visitNumber\": \"totalVisits\"}, inplace=True)\n",
    "    merged = merged.merge(total_visits)\n",
    "\n",
    "    print(\"Splitting back...\")\n",
    "    train = merged[:train_len]\n",
    "    test = merged[train_len:]\n",
    "    return train, test\n",
    "\n",
    "def preprocess_and_save(data_dir):\n",
    "    train = load(os.path.join(data_dir, \"train.csv\"))\n",
    "    test = load(os.path.join(data_dir, \"test.csv\"))\n",
    "    \n",
    "    train, test = process(train, test)\n",
    "    train.to_csv(os.path.join(data_dir, \"preprocessed_train.csv\"), index=False)\n",
    "    test.to_csv(os.path.join(data_dir, \"preprocessed_test.csv\"), index=False)    \n",
    "    \n",
    "\n",
    "# Call this to save the preprocessed data for later use\n",
    "# preprocess_and_save(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load(\"../data/train.csv\", nrows=100000)\n",
    "test = load(\"../data/test.csv\", nrows=10000)\n",
    "\n",
    "train, test = process(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categoricals with many values\n",
    "\n",
    "There are a number of categorical features with many different values. This is an issue, specifically in the cases where those categorical features are not ordinal, and therefore label encoding them does not make sense. The only choice we are left with, is OHE. However naively performing this step would add hundreds of columns for each original categorical feature - in the end yielding potentially thousands of super sparse features. What I would like to explore, is whether there exist **specific values** with predictive value significantly higher than average. For example, looking at the particular country might be a weak predictor. However there might be 5 specific countries with a huge revenue deviation from the average (and enough samples to consider this discrepancy statistically significant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = train['country']\n",
    "print(\"There are {} different countries in our dataset\".format(len(countries.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = {'target':['mean', 'count']}\n",
    "\n",
    "countries = train[[\"country\", \"target\"]].groupby(\"country\", as_index=False).agg(aggregations)\n",
    "countries.columns = [\"country\", \"targetMean\", \"occurenceCount\"]\n",
    "\n",
    "# Let's focus only on countries with multiple records to preserve some statistical significance\n",
    "keep = 10\n",
    "usual_countries = countries.sort_values(\"occurenceCount\", ascending=False).head(keep)\n",
    "\n",
    "global_average = train[\"target\"].mean()\n",
    "usual_countries[\"deviation\"] = usual_countries[\"targetMean\"] - global_average\n",
    "usual_countries.plot.bar(x=\"country\", y=\"deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA seems quite different\n",
    "\n",
    "What we can find here is that USA is very different to any other country with significant sample count. What we can do with this information? We instead of using OHE to code every country in our dataset, we can probably get away with a single boolean column: **is this record coming from the USA?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = train['city']\n",
    "print(\"There are {} different cities in our dataset\".format(len(cities.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = {'target':['mean', 'count']}\n",
    "\n",
    "cities = train[[\"city\", \"target\"]].groupby(\"city\", as_index=False).agg(aggregations)\n",
    "cities.columns = [\"city\", \"targetMean\", \"occurenceCount\"]\n",
    "\n",
    "# Let's focus only on countries with multiple records to preserve some statistical significance\n",
    "keep = 10\n",
    "usual_cities = cities.sort_values(\"occurenceCount\", ascending=False).head(keep)\n",
    "\n",
    "global_average = train[\"target\"].mean()\n",
    "usual_cities[\"deviation\"] = usual_cities[\"targetMean\"] - global_average\n",
    "usual_cities.plot.bar(x=\"city\", y=\"deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about the cities?\n",
    "\n",
    "Here we can see some pretty strong deviations. However we need to note that the top ones come from US cities, so part of the variance these cities explain, is already included in the information that they belong to the USA. However their deviation is considerably higher than that of USA alone (1.0 vs 0.3) so including those columns might still be beneficial. The deviation we see is actually very distorted because of the USA outlier. Perhaps it would make more sense to only focus on the deviation from the average of non-USA cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_not_us = train[train['country'] != \"United States\"]\n",
    "train_us = train[train['country'] == \"United States\"]\n",
    "\n",
    "outside_us_avg = train_not_us['target'].mean()\n",
    "us_avg = train_us[\"target\"].mean()\n",
    "\n",
    "print(\"Average in US: {}\\nAverage outside US: {}\".format(us_avg, outside_us_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's then repeat out analysis but this time separatly for the pieces of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = {'target':['mean', 'count']}\n",
    "\n",
    "# US case\n",
    "def city_deviation(df, title=\"Deviation per city\"):\n",
    "    cities = df[[\"city\", \"target\"]].groupby(\"city\", as_index=False).agg(aggregations)\n",
    "    cities.columns = [\"city\", \"targetMean\", \"occurenceCount\"]\n",
    "\n",
    "    # Let's focus only on countries with multiple records to preserve some statistical significance\n",
    "    keep = 10\n",
    "    usual_cities = cities.sort_values(\"occurenceCount\", ascending=False).head(keep)\n",
    "\n",
    "    global_average = df[\"target\"].mean()\n",
    "    usual_cities[\"deviation\"] = usual_cities[\"targetMean\"] - global_average\n",
    "    ax = usual_cities.plot.bar(x=\"city\", y=\"deviation\", title=title, rot=45, legend=False)\n",
    "    ax.set_xlabel(\"City\")\n",
    "    ax.set_ylabel(\"Deviation\")\n",
    "    \n",
    "city_deviation(train_us, title=\"Deviation from the mean - US\")\n",
    "city_deviation(train_not_us, title=\"Deviation from the mean - Rest of the world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much better!\n",
    "\n",
    "Now we can clearly see what information should be included besides the country (or to be exact, whether or not the country is the US). For example it makes no sense to include Los Angeles or Mountain View even though they deviate from the global average, because all this deviation is explained by the fact that they exist in the US! Instead we should include Chicago, New York, Austin, Seattle and maybe Palo Alto. And as we can see the deviations are much smaller outside the US, with the exception of Toronto which MUST be included.\n",
    "\n",
    "### Food for thought\n",
    "It makes sense that deviations outside the US are smaller because the target itself is considerable lower. Perhaps we should look at relative deviations instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = ohe_explicit(train)\n",
    "check.head()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
